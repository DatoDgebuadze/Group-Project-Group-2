{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79ce060-7b0d-4b01-bf53-d4ba360d86f6",
   "metadata": {},
   "source": [
    "# COMP2006: Group Project\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To successfully complete this project, you need to collect and process data, then train and evaluate at least two machine learning models and lastly deploy them to a website. Please see below for further details:\n",
    "\n",
    "> Python (or a python package) is to be used everywhere it possibly can!\n",
    "\n",
    "**Data Collection**\n",
    "\n",
    "Data can be collected (legally) from anywhere. You may use data that you already have; or from sites that allow you to download the data, for example, [UCI Machine Learning Repository](https://archive.ics.uci.edu/) or [Kaggle Datasets](https://www.kaggle.com/datasets); or via web scraping; or via an API. We can restrict ourselves to data that would fit nicely into a spreadsheet. The content and amount of data are not the main consideration, as long as the data has:\n",
    "- at least 10 variables\n",
    "- three or more data types\n",
    "- two or more problems: missing data, inconsistencies, errors, categorical data that needs to be converted to numeric, entries like text that need to be converted into proper features, etc. \n",
    "- if the data does not have enough problems, you can substitute one problem for feature engineering (creating new features from the original features)\n",
    "\n",
    "We are not concerned with acquiring *huge* datasets or creating super accurate models, but more with the process of creating a proper pipeline for machine learning and, for any model deployed, having a reliable estimate of its performance. Although, some effort should go into improving an initial model. \n",
    "\n",
    "How many datasets do you need?\n",
    " - Groups of 2 need **two** datasets\n",
    " - Group of 3 needs **three** datasets\n",
    "\n",
    "**Database**\n",
    "\n",
    "After the data has been collected and processed, it should be stored in a SQLite database. At a minimum, each dataset should have its own table. Database and table creation and data insertion can be done either with the `sqlite3` package or with `Pandas`. Both SQLite and `sqlite3` come with Python. \n",
    "\n",
    "**Machine Learning Models**\n",
    "\n",
    "For each dataset you should train, evaluate, and save a machine learning model:\n",
    "- one model should be for a *classification* problem\n",
    "- the other model should be for a *regression* problem\n",
    "- Group of 3: you should have 2 of one type\n",
    "\n",
    "A *validation* dataset must be used to either select between models, or to choose between hyperparameter values of a single model. \n",
    "\n",
    "A *test* dataset must be used to evaluate the performance of the final chosen model. \n",
    "\n",
    "**Website**\n",
    "\n",
    "The final models should be presented to an end-user through a website. (Deployment need only be to *localhost*). The website must be done using a Python \"web framework\", e.g., *flask*, *Django*, *streamlit*.  \n",
    "\n",
    "The website should have:\n",
    " - a *Welcome* page that describes your project\n",
    " - an *About* page for each dataset that provides:\n",
    "     - the source of the dataset\n",
    "     - definition of each variable in the dataset\n",
    "     - a view of a sample of the dataset used for training (pulled from the database)\n",
    " - a page for each machine learning model that:\n",
    "    - identifies the model being used, with a brief description\n",
    "    - allows the end-user to enter their own data to get a prediction\n",
    "\n",
    "**Readme.md**\n",
    "\n",
    "This file should present the reader with a basic description of your project and how they can use it. \n",
    "\n",
    "**Requirements.txt**\n",
    "\n",
    "This file contains all packages necessary to run your code. This file should allow the user to install all necessary packages via the command: `pip install -r requirements.txt`\n",
    "\n",
    "\n",
    "## Structure\n",
    "\n",
    "- All project related code in a single Github repository\n",
    "- All code in the repository is only FINAL code\n",
    "- The repository structure is\n",
    "    - main folder\n",
    "        - data collection \n",
    "        - data processing \n",
    "        - database\n",
    "        - models\n",
    "            - model 1\n",
    "            - model 2\n",
    "            - model 3 (if required)\n",
    "        - website\n",
    "        - Readme.md\n",
    "        - requirements.txt\n",
    "- Each subfolder should be logically organized\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submission consists of uploading a link to the Github repository containing all the code for your project. There should be one submission per group. \n",
    "\n",
    "Example: `https://github.com/markcassar/COMP2006_project_Group_8`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a4e2c-4fbb-4ab2-a073-bb21b65f1e4f",
   "metadata": {},
   "source": [
    "## Security\n",
    "Using an API is still allowed, just not required. If you choose to use an API, then \n",
    "\n",
    "> please DO NOT include your API Key in your GitHub repository\n",
    "\n",
    "You will be creating a public GitHub repository for your project, which means anyone can access and use your any code or data that is in it. Many API providers will require you to register and create an API Key. When accessing data through the API, you need to authenticate using your API Key before any data will be returned from an API call. \n",
    "\n",
    "To keep you API Key(s) safe, please do the following:\n",
    " - create a `credentials.py` file that stores the value of your key(s) in Python variables\n",
    " - add `credentials.py` to the `.gitignore` file of your repository so GitHub does not automatically track any changes to this file\n",
    " - in your code, you can access your keys via import:\n",
    " \n",
    " ```python\n",
    " import credentials\n",
    " \n",
    " weather_api_key = credentials.weather_api_key\n",
    " ```\n",
    "In this way, anyone accessing your GitHub repository will not be able to access your personal API account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f4a094a-cde8-43bc-9677-e131b1e8c2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17647058823529413\n",
      "      Year  Home Score  Away Score\n",
      "0   1960.0         0.0         3.0\n",
      "1   1960.0         4.0         5.0\n",
      "2   1960.0         0.0         2.0\n",
      "3   1960.0         2.0         1.0\n",
      "4   1964.0         0.0         3.0\n",
      "5   1964.0         2.0         1.0\n",
      "6   1964.0         3.0         1.0\n",
      "7   1964.0         2.0         1.0\n",
      "8   1968.0         0.0         1.0\n",
      "9   1968.0         0.0         0.0\n",
      "10  1968.0         2.0         0.0\n",
      "11  1968.0         1.0         1.0\n",
      "12  1968.0         2.0         0.0\n",
      "13  1972.0         1.0         2.0\n",
      "14  1972.0         0.0         1.0\n",
      "15  1972.0         2.0         1.0\n",
      "16  1972.0         3.0         0.0\n",
      "17  1976.0         3.0         1.0\n",
      "18  1976.0         2.0         4.0\n",
      "19  1976.0         2.0         3.0\n",
      "20  1976.0         2.0         2.0\n",
      "21  1980.0         0.0         1.0\n",
      "22  1980.0         NaN         0.0\n",
      "23  1980.0         1.0         1.0\n",
      "24  1980.0         0.0         0.0\n",
      "25  1980.0         3.0         2.0\n",
      "26  1980.0         1.0         3.0\n",
      "27  1980.0         2.0         1.0\n",
      "28  1980.0         1.0         0.0\n",
      "29  1980.0         0.0         0.0\n",
      "30  1980.0         1.0         1.0\n",
      "31  1980.0         0.0         0.0\n",
      "32  1980.0         1.0         2.0\n",
      "33  1980.0         1.0         1.0\n",
      "34  1980.0         1.0         2.0\n",
      "35  1984.0         1.0         0.0\n",
      "36  1984.0         2.0         0.0\n",
      "37  1984.0         0.0         0.0\n",
      "38  1984.0         NaN         1.0\n",
      "39  1984.0         5.0         0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sqlite3\n",
    "# Load the dataset\n",
    "euros_read_file = pd.read_csv(\"../dataset/euros.csv\")\n",
    "conn = sqlite3.connect('../database/pd_data.db')\n",
    "\n",
    "# Define base features and target variable\n",
    "target = 'Winning Team'\n",
    " \n",
    "# Drop the 'Date' column\n",
    "euros_read_file.drop(columns=['Date'], inplace=True)\n",
    " \n",
    "#Createing missing values in all numeric columns\n",
    "missing_percentage = 0.05  # 5% missing in all numeric features here\n",
    "for column in euros_read_file.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    num_missing = int(len(euros_read_file) * missing_percentage)\n",
    "    missing_indices = euros_read_file[column].sample(n=num_missing).index\n",
    "    euros_read_file.loc[missing_indices, column] = np.nan\n",
    " \n",
    " \n",
    "dataframe_numeric_features = euros_read_file.select_dtypes(include=['int64', 'float64'])\n",
    " \n",
    "# Splitting data into features and target.\n",
    "X = dataframe_numeric_features\n",
    "y = euros_read_file[target]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "# decided to use randomForestClassifier with 150 decision trees\n",
    "model = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    " \n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    " \n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    " \n",
    "# Display the updated DataFrame\n",
    "print(dataframe_numeric_features.head(40))\n",
    "\n",
    "euros_read_file.to_sql('pd_euros', conn, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "# conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29e15e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning Team\n",
      "9     63\n",
      "13    29\n",
      "29    25\n",
      "17    25\n",
      "12    22\n",
      "22    21\n",
      "18    21\n",
      "10    16\n",
      "25    13\n",
      "6     13\n",
      "8     11\n",
      "3     11\n",
      "5      9\n",
      "30     7\n",
      "14     5\n",
      "7      5\n",
      "34     5\n",
      "32     5\n",
      "31     4\n",
      "21     3\n",
      "33     3\n",
      "23     2\n",
      "16     2\n",
      "28     2\n",
      "15     2\n",
      "35     2\n",
      "26     2\n",
      "2      2\n",
      "24     1\n",
      "27     1\n",
      "20     1\n",
      "4      1\n",
      "19     1\n",
      "1      1\n",
      "11     1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\810368187.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  read_file1[target][read_file1[target] == 'class0'] = 'class1'\n",
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\810368187.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  read_file1[target][read_file1[target] == 'class0'] = 'class1'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m y \u001b[38;5;241m=\u001b[39m read_file1[target]  \u001b[38;5;66;03m# Target variable\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.33\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Instantiate and fit the RandomForestClassifier\u001b[39;00m\n\u001b[0;32m     38\u001b[0m random_forest \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n",
      "File \u001b[1;32mc:\\Users\\Pc\\Desktop\\ML\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Pc\\Desktop\\ML\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2681\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2677\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[0;32m   2679\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m-> 2681\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2684\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2685\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2686\u001b[0m     )\n\u001b[0;32m   2687\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Pc\\Desktop\\ML\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1749\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1719\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m \n\u001b[0;32m   1721\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m-> 1749\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32mc:\\Users\\Pc\\Desktop\\ML\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2150\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   2148\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 2150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2155\u001b[0m     )\n\u001b[0;32m   2157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[0;32m   2158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[0;32m   2161\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pandas.api.types import is_string_dtype, is_categorical_dtype\n",
    "\n",
    "# Load the dataset\n",
    "read_file1 = pd.read_csv(\"../dataset/euros.csv\")\n",
    "\n",
    "# Define base features and target variable\n",
    "target = 'Winning Team'\n",
    "dataframe_numeric_features = read_file1.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Normalize missing values\n",
    "read_file1.fillna(np.nan, inplace=True)\n",
    "\n",
    "# Convert string columns to categorical and ordinal encode\n",
    "for col in read_file1.columns:\n",
    "    if is_string_dtype(read_file1[col]):\n",
    "        read_file1[col] = read_file1[col].astype('category').cat.codes + 1\n",
    "\n",
    "# Separate features and target variable\n",
    "X = read_file1.drop(target, axis=1)  # Features\n",
    "y = read_file1[target]  # Target variable\n",
    "# View count of each class\n",
    "y.value_counts()\n",
    "# print(read_file1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "#I set a random_state; this ensures that if I have to rerun my code, I’ll get the exact same train-test split\n",
    "\n",
    "#instantiating and fitting training  data in RFCLAssifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas.api.types import is_string_dtype\n",
    " \n",
    "# Load the dataset\n",
    "read_file1 = pd.read_csv(\"./dataset/euros.csv\")\n",
    " \n",
    "# Define base features and target variable\n",
    "target = 'Winning Team'\n",
    "dataframe_numeric_features = read_file1.select_dtypes(include=['int64', 'float64'])\n",
    " \n",
    "# Normalize missing values\n",
    "read_file1.fillna(np.nan, inplace=True)\n",
    " \n",
    "# Convert string columns to categorical and ordinal encode\n",
    "for col in read_file1.columns:\n",
    "    if is_string_dtype(read_file1[col]):\n",
    "        read_file1[col] = read_file1[col].astype('category').cat.codes + 1\n",
    " \n",
    "# Check value counts of each class\n",
    "print(read_file1[target].value_counts())\n",
    " \n",
    "# Combine classes if needed (adjust based on your actual class labels)\n",
    "# For example, if 'class0' has only one instance, combine it with 'class1'\n",
    "read_file1[target][read_file1[target] == 'class0'] = 'class1'\n",
    " \n",
    "# Separate features and target variable\n",
    "X = read_file1.drop(target, axis=1)  # Features\n",
    "y = read_file1[target]  # Target variable\n",
    " \n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    " \n",
    "# Instantiate and fit the RandomForestClassifier\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)\n",
    " \n",
    "# Make predictions on the test set\n",
    "y_pred_test = random_forest.predict(X_test)\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas.api.types import is_string_dtype\n",
    " \n",
    "# Load the dataset\n",
    "read_file1 = pd.read_csv(\"./dataset/euros.csv\")\n",
    " \n",
    "# Define base features and target variable\n",
    "target = 'Winning Team'\n",
    "dataframe_numeric_features = read_file1.select_dtypes(include=['int64', 'float64'])\n",
    " \n",
    "# Normalize missing values\n",
    "read_file1.fillna(np.nan, inplace=True)\n",
    " \n",
    "# Convert string columns to categorical and ordinal encode\n",
    "for col in read_file1.columns:\n",
    "    if is_string_dtype(read_file1[col]):\n",
    "        read_file1[col] = read_file1[col].astype('category').cat.codes + 1\n",
    " \n",
    "# Check value counts of each class\n",
    "print(read_file1[target].value_counts())\n",
    " \n",
    "# Combine classes if needed (adjust based on your actual class labels)\n",
    "# For example, if 'class0' has only one instance, combine it with 'class1'\n",
    "read_file1[target][read_file1[target] == 'class0'] = 'class1'\n",
    " \n",
    "# Separate features and target variable\n",
    "X = read_file1.drop(target, axis=1)  # Features\n",
    "y = read_file1[target]  # Target variable\n",
    " \n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    " \n",
    "# Instantiate and fit the RandomForestClassifier\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(X_train, y_train)\n",
    " \n",
    "# Make predictions on the test set\n",
    "y_pred_test = random_forest.predict(X_test)\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "y_pred_test = random_forest.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e564561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "euros_read_file = pd.read_csv(\"../dataset/euros.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0aeba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Year  Home Score  Away Score\n",
      "0    1960.0         0.0         3.0\n",
      "1    1960.0         4.0         5.0\n",
      "2    1960.0         0.0         2.0\n",
      "3    1960.0         2.0         1.0\n",
      "4    1964.0         0.0         3.0\n",
      "..      ...         ...         ...\n",
      "332  2021.0         1.0         2.0\n",
      "333  2021.0         0.0         4.0\n",
      "334  2021.0         1.0         1.0\n",
      "335  2021.0         2.0         1.0\n",
      "336  2021.0         0.0         1.0\n",
      "\n",
      "[337 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataframe_numeric_features.fillna(0, inplace=True)\n",
    "\n",
    "print(dataframe_numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15991146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\1179292920.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(euros_read_file[col]):\n",
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\1179292920.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(euros_read_file[col]):\n",
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\1179292920.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(euros_read_file[col]):\n",
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\1179292920.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(euros_read_file[col]):\n",
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\1179292920.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(euros_read_file[col]):\n",
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\1179292920.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(euros_read_file[col]):\n",
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\1179292920.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(euros_read_file[col]):\n",
      "C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_5268\\1179292920.py:17: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(euros_read_file[col]):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_categorical_dtype\n",
    "# for some reason is categorical dtype has this issues though they are talking on depricating\n",
    "# this in next feature this should be working for now\n",
    "# \n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "# Assuming euros_read_file is your DataFrame\n",
    "\n",
    "# Loop through each column and check if it's a string dtype\n",
    "for col in euros_read_file.columns:\n",
    "    if is_string_dtype(euros_read_file[col]):\n",
    "        euros_read_file[col] = euros_read_file[col].astype('category').cat.as_ordered()\n",
    "\n",
    "# Loop through each column and check if it's a categorical dtype\n",
    "for col in euros_read_file.columns:\n",
    "    if is_categorical_dtype(euros_read_file[col]):\n",
    "        euros_read_file[col] = euros_read_file[col].cat.codes + 1\n",
    "\n",
    "# Check unique values before and after conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3921e204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of      Year  Date  Home Team  Away Team  Home Score  Away Score  Shootout  \\\n",
       "0    1960     1          7         26         0.0         3.0     False   \n",
       "1    1960     1         11         37         4.0         5.0     False   \n",
       "2    1960     2         11          7         0.0         2.0     False   \n",
       "3    1960     3         25         37         2.0         1.0     False   \n",
       "4    1964     4          8         26         0.0         3.0     False   \n",
       "..    ...   ...        ...        ...         ...         ...       ...   \n",
       "332  2021   184          6          8         1.0         2.0     False   \n",
       "333  2021   184         34          9         0.0         4.0     False   \n",
       "334  2021   185         16         31         1.0         1.0      True   \n",
       "335  2021   186          9          8         2.0         1.0     False   \n",
       "336  2021   187          9         16         1.0         1.0      True   \n",
       "\n",
       "     Tournament  City  Country  Neutral Venue  Winning Team first_shooter  \\\n",
       "0             1    52        6           True            25           NaN   \n",
       "1             1    61        6          False            35           NaN   \n",
       "2             1    52        6          False             7           NaN   \n",
       "3             1    61        6           True            25           NaN   \n",
       "4             1     6       16           True            25           NaN   \n",
       "..          ...   ...      ...            ...           ...           ...   \n",
       "332           1     5        2           True             8           NaN   \n",
       "333           1    64        9           True            10           NaN   \n",
       "334           1    46        5           True            17         Italy   \n",
       "335           1    46        5          False            10           NaN   \n",
       "336           1    46        5          False            17         Italy   \n",
       "\n",
       "     Losing Team  \n",
       "0              7  \n",
       "1             12  \n",
       "2             12  \n",
       "3             38  \n",
       "4              8  \n",
       "..           ...  \n",
       "332            6  \n",
       "333           36  \n",
       "334           32  \n",
       "335            8  \n",
       "336           10  \n",
       "\n",
       "[337 rows x 14 columns]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euros_read_file.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4d0d41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16176470588235295\n"
     ]
    }
   ],
   "source": [
    "combined_features = pd.concat([euros_read_file, dataframe_numeric_features], axis=1)\n",
    "target = 'Winning Team'\n",
    "X = dataframe_numeric_features\n",
    "y = euros_read_file[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "# decided to use randomForestClassifier with 150 decision trees\n",
    "model = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    " \n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "# print(y_pred)\n",
    "# Re evaluation Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c0b20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# fuel_read_file = pd.read_csv(\"my2024-fuel-consumption-ratings 1.csv\")\n",
    "\n",
    "# print(fuel_read_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
