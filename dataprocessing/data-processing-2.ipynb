{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79ce060-7b0d-4b01-bf53-d4ba360d86f6",
   "metadata": {},
   "source": [
    "# COMP2006: Group Project\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To successfully complete this project, you need to collect and process data, then train and evaluate at least two machine learning models and lastly deploy them to a website. Please see below for further details:\n",
    "\n",
    "> Python (or a python package) is to be used everywhere it possibly can!\n",
    "\n",
    "**Data Collection**\n",
    "\n",
    "Data can be collected (legally) from anywhere. You may use data that you already have; or from sites that allow you to download the data, for example, [UCI Machine Learning Repository](https://archive.ics.uci.edu/) or [Kaggle Datasets](https://www.kaggle.com/datasets); or via web scraping; or via an API. We can restrict ourselves to data that would fit nicely into a spreadsheet. The content and amount of data are not the main consideration, as long as the data has:\n",
    "- at least 10 variables\n",
    "- three or more data types\n",
    "- two or more problems: missing data, inconsistencies, errors, categorical data that needs to be converted to numeric, entries like text that need to be converted into proper features, etc. \n",
    "- if the data does not have enough problems, you can substitute one problem for feature engineering (creating new features from the original features)\n",
    "\n",
    "We are not concerned with acquiring *huge* datasets or creating super accurate models, but more with the process of creating a proper pipeline for machine learning and, for any model deployed, having a reliable estimate of its performance. Although, some effort should go into improving an initial model. \n",
    "\n",
    "How many datasets do you need?\n",
    " - Groups of 2 need **two** datasets\n",
    " - Group of 3 needs **three** datasets\n",
    "\n",
    "**Database**\n",
    "\n",
    "After the data has been collected and processed, it should be stored in a SQLite database. At a minimum, each dataset should have its own table. Database and table creation and data insertion can be done either with the `sqlite3` package or with `Pandas`. Both SQLite and `sqlite3` come with Python. \n",
    "\n",
    "**Machine Learning Models**\n",
    "\n",
    "For each dataset you should train, evaluate, and save a machine learning model:\n",
    "- one model should be for a *classification* problem\n",
    "- the other model should be for a *regression* problem\n",
    "- Group of 3: you should have 2 of one type\n",
    "\n",
    "A *validation* dataset must be used to either select between models, or to choose between hyperparameter values of a single model. \n",
    "\n",
    "A *test* dataset must be used to evaluate the performance of the final chosen model. \n",
    "\n",
    "**Website**\n",
    "\n",
    "The final models should be presented to an end-user through a website. (Deployment need only be to *localhost*). The website must be done using a Python \"web framework\", e.g., *flask*, *Django*, *streamlit*.  \n",
    "\n",
    "The website should have:\n",
    " - a *Welcome* page that describes your project\n",
    " - an *About* page for each dataset that provides:\n",
    "     - the source of the dataset\n",
    "     - definition of each variable in the dataset\n",
    "     - a view of a sample of the dataset used for training (pulled from the database)\n",
    " - a page for each machine learning model that:\n",
    "    - identifies the model being used, with a brief description\n",
    "    - allows the end-user to enter their own data to get a prediction\n",
    "\n",
    "**Readme.md**\n",
    "\n",
    "This file should present the reader with a basic description of your project and how they can use it. \n",
    "\n",
    "**Requirements.txt**\n",
    "\n",
    "This file contains all packages necessary to run your code. This file should allow the user to install all necessary packages via the command: `pip install -r requirements.txt`\n",
    "\n",
    "\n",
    "## Structure\n",
    "\n",
    "- All project related code in a single Github repository\n",
    "- All code in the repository is only FINAL code\n",
    "- The repository structure is\n",
    "    - main folder\n",
    "        - data collection \n",
    "        - data processing \n",
    "        - database\n",
    "        - models\n",
    "            - model 1\n",
    "            - model 2\n",
    "            - model 3 (if required)\n",
    "        - website\n",
    "        - Readme.md\n",
    "        - requirements.txt\n",
    "- Each subfolder should be logically organized\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submission consists of uploading a link to the Github repository containing all the code for your project. There should be one submission per group. \n",
    "\n",
    "Example: `https://github.com/markcassar/COMP2006_project_Group_8`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a4e2c-4fbb-4ab2-a073-bb21b65f1e4f",
   "metadata": {},
   "source": [
    "## Security\n",
    "Using an API is still allowed, just not required. If you choose to use an API, then \n",
    "\n",
    "> please DO NOT include your API Key in your GitHub repository\n",
    "\n",
    "You will be creating a public GitHub repository for your project, which means anyone can access and use your any code or data that is in it. Many API providers will require you to register and create an API Key. When accessing data through the API, you need to authenticate using your API Key before any data will be returned from an API call. \n",
    "\n",
    "To keep you API Key(s) safe, please do the following:\n",
    " - create a `credentials.py` file that stores the value of your key(s) in Python variables\n",
    " - add `credentials.py` to the `.gitignore` file of your repository so GitHub does not automatically track any changes to this file\n",
    " - in your code, you can access your keys via import:\n",
    " \n",
    " ```python\n",
    " import credentials\n",
    " \n",
    " weather_api_key = credentials.weather_api_key\n",
    " ```\n",
    "In this way, anyone accessing your GitHub repository will not be able to access your personal API account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4a094a-cde8-43bc-9677-e131b1e8c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # imports what we want to run this code\n",
    "# import pandas as pd\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# # reads the csv file\n",
    "# fuel_read_file = pd.read_csv(\"./dataset/my2024-fuel-consumption-ratings.csv\")\n",
    " \n",
    " \n",
    "# X = fuel_read_file[['Engine size (L)', 'Cylinders', 'Highway (L/100 km)']]\n",
    "# y = fuel_read_file['City (L/100 km)']\n",
    " \n",
    " \n",
    "# regressor = KNeighborsRegressor(n_neighbors=5)\n",
    " \n",
    " \n",
    "# regressor.fit(X, y)\n",
    " \n",
    " \n",
    "# new_data = pd.DataFrame({\n",
    "#     'Engine size (L)': [2.0], 'Cylinders': [4], 'Highway (L/100 km)': [6.5]\n",
    "# })\n",
    " \n",
    "# predicted_city_consumption = regressor.predict(new_data)\n",
    "# print(f\"Predicted City Fuel Consumption: {predicted_city_consumption[0]:.1f} L/100 km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e564561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE of 10 runs: 0.16501607643358057\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "read_file = pd.read_csv(\"../dataset/my2024-fuel-consumption-ratings.csv\")\n",
    "dataframe_numeric_features = read_file.select_dtypes(include=['int64', 'float64'])\n",
    "dataframe_numeric_features.fillna(0, inplace=True)\n",
    "# Createing missing values in all numeric columns\n",
    "missing_percentage = 0.05  # 5% missing in all numeric features here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define target and features\n",
    "target = 'City (L/100 km)'\n",
    "X = dataframe_numeric_features.drop(columns=[target])\n",
    "Y = dataframe_numeric_features[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "mae_scores = []\n",
    "\n",
    "\n",
    "# Number of runs\n",
    "num_runs = 10\n",
    "\n",
    "for ii in range(num_runs):\n",
    "    # Random Forest Regressor\n",
    "    random_Forest = RandomForestRegressor(n_estimators=150)\n",
    "    random_Forest.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    Y_pred = random_Forest.predict(X_test)\n",
    "    \n",
    "    # MAE\n",
    "    mae_scores.append(mean_absolute_error(Y_test, Y_pred))\n",
    "# Calculate average scores\n",
    "mean_mae = np.mean(mae_scores)\n",
    "\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Average MAE of {num_runs} runs:\", mean_mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aeba92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15991146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b20fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
